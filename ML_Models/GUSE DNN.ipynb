{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Irony Using Google Universal Sentence Encoder\n",
    "In this notebook, I will use Google's Universal Sentence Encoder and an artificial neural network to detect irony."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements\n",
    "Below are the necessary packages for running this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0505 19:15:44.339353 17336 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import keras.backend as K\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import *\n",
    "from keras.regularizers import *\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "The code below loads data from the `.pickle` files generated by the data collection code. The next cell reshapes the data so that the feature and label tensors are the correct dimensions to be fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ironic examples: 320\n",
      "Non ironic examples: 807\n",
      "Full set (for validation): 1127\n"
     ]
    }
   ],
   "source": [
    "goodreads_irony_f = open(\"goodreads_irony_edited.pickle\", mode=\"rb\")\n",
    "goodreads_knowledge_f = open(\"goodreads_knowledge_keep.pickle\", mode=\"rb\")\n",
    "goodreads_metaphor_f = open(\"goodreads_metaphor_keep.pickle\", mode=\"rb\")\n",
    "\n",
    "goodreads_irony = pickle.load(goodreads_irony_f)\n",
    "goodreads_knowledge = pickle.load(goodreads_knowledge_f)\n",
    "goodreads_metaphor = pickle.load(goodreads_metaphor_f)\n",
    "\n",
    "goodreads_irony_f.close()\n",
    "goodreads_knowledge_f.close()\n",
    "goodreads_metaphor_f.close()\n",
    "\n",
    "ironic = goodreads_irony\n",
    "ironic_labels = np.ones(len(ironic))\n",
    "non_ironic = goodreads_knowledge + goodreads_metaphor\n",
    "non_ironic_labels = np.zeros(len(non_ironic))\n",
    "full_set = ironic + non_ironic\n",
    "full_labels = np.concatenate([ironic_labels, non_ironic_labels])\n",
    "\n",
    "ironic_size = len(ironic)\n",
    "non_ironic_size = len(non_ironic)\n",
    "full_set_size = len(full_set)\n",
    "\n",
    "print(\"Ironic examples: \" + str(ironic_size))\n",
    "print(\"Non ironic examples: \" + str(non_ironic_size))\n",
    "print(\"Full set (for validation): \" + str(full_set_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.reshape(train_features, (train_features.shape[0],1))\n",
    "test_features = np.reshape(test_features, (test_features.shape[0],1))\n",
    "x = np.reshape(full_set_np, (full_set_np.shape[0],1))\n",
    "y = full_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Google's Universal Sentence Encoder\n",
    "The code below loads the GUSE embedding with the command `embed = hub.Module('./embeddings/GUSE')`. The sentence \"The quick brown fox jumped over the lazy dog\" is passed into the encoder and the resulting 512 by 1 vector is displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0505 19:15:53.322526 17336 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.64562017e-02,  6.23100176e-02,  3.46776913e-03,\n",
       "         4.70204279e-03,  3.23413201e-02,  9.14165005e-03,\n",
       "        -3.01914979e-02, -3.84491310e-02, -3.57649252e-02,\n",
       "        -4.20018146e-03, -1.91205703e-02,  2.40280163e-02,\n",
       "         5.23959063e-02,  2.04604957e-02,  3.24646309e-02,\n",
       "         8.14204291e-02, -3.86404321e-02,  1.64812542e-02,\n",
       "         1.34157818e-02, -3.47102135e-02,  7.75472447e-02,\n",
       "        -7.51131922e-02,  1.21427458e-02,  4.46278341e-02,\n",
       "         7.48834983e-02, -1.19225457e-02, -2.63228826e-02,\n",
       "         1.70647651e-02,  7.75867552e-02,  4.95012477e-02,\n",
       "         3.56119871e-02,  4.83054519e-02, -1.33931926e-02,\n",
       "        -2.03491952e-02,  1.61516182e-02, -6.49256110e-02,\n",
       "        -3.63756418e-02, -3.20054255e-02,  4.01207134e-02,\n",
       "         5.42915612e-02, -4.16404940e-02, -5.53556010e-02,\n",
       "        -6.92512095e-02,  4.85552102e-03,  1.49620827e-02,\n",
       "         2.48402003e-02,  1.57571007e-02,  4.47167791e-02,\n",
       "        -7.07037523e-02, -7.17832223e-02,  5.00952639e-03,\n",
       "        -2.59589087e-02, -5.78592904e-02,  2.31313370e-02,\n",
       "         7.53127411e-02,  4.22311127e-02, -1.19252764e-02,\n",
       "         5.68130985e-02,  5.32350363e-03, -2.40766387e-02,\n",
       "        -2.71194614e-02, -1.23549681e-02, -6.05905168e-02,\n",
       "        -1.94701217e-02,  5.49283922e-02, -3.27294543e-02,\n",
       "        -9.29304340e-04,  4.98049296e-02,  4.61435951e-02,\n",
       "         6.48857653e-02, -2.48945551e-03, -8.21273476e-02,\n",
       "         5.40077612e-02, -7.62300938e-02, -4.75528352e-02,\n",
       "        -3.54864188e-02,  7.70851523e-02,  3.46920565e-02,\n",
       "         5.68792820e-02,  7.59117305e-02, -7.16259517e-03,\n",
       "        -8.50538984e-02,  2.76591312e-02, -3.63997780e-02,\n",
       "         5.02404310e-02,  8.57603103e-02,  2.21230891e-02,\n",
       "         4.22035232e-02,  3.18021886e-02, -4.18099612e-02,\n",
       "        -9.60001722e-03, -5.96760325e-02, -5.65165654e-02,\n",
       "         4.37962413e-02, -8.60823169e-02,  6.06312118e-02,\n",
       "        -2.10551303e-02, -3.41948196e-02,  1.73334964e-02,\n",
       "         4.63457033e-02, -3.17540020e-02, -7.34451935e-02,\n",
       "        -6.40494302e-02,  1.19439578e-02, -2.39952859e-02,\n",
       "        -6.31210953e-02, -7.31313601e-02,  1.12030320e-02,\n",
       "         2.50914060e-02, -6.42428026e-02,  6.04019500e-03,\n",
       "        -5.66494986e-02,  4.86478992e-02, -2.28352528e-02,\n",
       "         1.16279554e-02, -6.29272163e-02,  3.89324948e-02,\n",
       "         1.15403673e-03, -8.13435316e-02,  1.51245492e-02,\n",
       "         3.85859683e-02, -4.73999940e-02,  5.69426175e-03,\n",
       "        -6.32515401e-02,  6.23311428e-03, -4.65926602e-02,\n",
       "         4.33222912e-02,  1.46401450e-02, -5.87723181e-02,\n",
       "         5.34503646e-02, -1.35761788e-02,  5.27061448e-02,\n",
       "         9.28275287e-03, -1.61010344e-02,  6.96154833e-02,\n",
       "        -5.09125106e-02,  3.11805774e-02,  3.23532932e-02,\n",
       "        -3.61187905e-02,  2.44922079e-02, -4.39777747e-02,\n",
       "        -5.07643446e-02,  3.91095765e-02, -1.77525450e-02,\n",
       "         6.39121160e-02,  1.66598130e-02,  4.55404120e-03,\n",
       "        -1.29464676e-03,  8.22825078e-03,  6.50314018e-02,\n",
       "        -1.32460836e-02,  8.32513273e-02, -3.42942476e-02,\n",
       "         1.78477373e-02, -8.13246295e-02, -3.97540405e-02,\n",
       "        -1.32131353e-02, -5.71311228e-02,  2.02368982e-02,\n",
       "        -4.40498106e-02, -3.79994810e-02, -1.86461564e-02,\n",
       "         4.07331623e-02, -2.77765258e-03,  7.28997067e-02,\n",
       "         8.29713494e-02, -2.67154556e-02,  5.88389076e-02,\n",
       "         5.46146929e-02,  5.18099479e-02, -5.92902899e-02,\n",
       "         5.34798056e-02,  3.86380330e-02, -3.24837528e-02,\n",
       "        -5.70955165e-02,  9.44633968e-03, -6.27198070e-02,\n",
       "        -3.65421772e-02, -2.33383570e-02,  3.50028500e-02,\n",
       "         4.38573956e-02, -4.85609993e-02,  1.84226874e-02,\n",
       "         2.01100893e-02, -1.02894260e-02,  2.86924522e-02,\n",
       "         8.15729052e-02,  1.87505130e-02,  3.88825648e-02,\n",
       "        -3.92058901e-02,  3.25564072e-02,  1.37221543e-02,\n",
       "        -1.87033713e-02,  5.82602574e-04, -1.98195986e-02,\n",
       "         4.49058935e-02,  4.31735925e-02,  5.82612585e-03,\n",
       "        -3.06446441e-02, -3.99568444e-03,  5.59397601e-02,\n",
       "         2.04245821e-02, -5.58269769e-02,  2.12360602e-02,\n",
       "         2.95248684e-02,  1.50440994e-03,  4.53147069e-02,\n",
       "         1.93627905e-02,  7.12841824e-02, -1.87540916e-03,\n",
       "        -2.23918096e-03, -2.77270866e-03, -5.37293255e-02,\n",
       "         9.34291072e-03, -7.37183988e-02,  3.14655490e-02,\n",
       "         1.18605432e-03,  3.71425115e-02,  2.24868804e-02,\n",
       "        -7.69365802e-02, -1.60495006e-02, -4.29585613e-02,\n",
       "        -8.72697029e-03, -6.05771281e-02, -5.13884984e-02,\n",
       "        -2.42714249e-02,  5.57295531e-02, -5.09858094e-02,\n",
       "         6.49150228e-03,  7.72769377e-03, -4.97083813e-02,\n",
       "        -9.90352780e-03, -7.74186179e-02,  3.16280406e-03,\n",
       "        -1.10710145e-03, -6.82533979e-02,  6.45895079e-02,\n",
       "         3.85526009e-02,  7.03889057e-02, -4.73733991e-02,\n",
       "         4.24536597e-03,  7.10388348e-02, -5.26830442e-02,\n",
       "        -1.78547557e-02,  3.46471667e-02,  1.37007553e-02,\n",
       "         7.50325248e-02,  1.58655327e-02,  4.82993014e-02,\n",
       "        -5.28091267e-02, -2.17354707e-02,  3.33840586e-02,\n",
       "        -7.30166063e-02, -2.03380305e-02,  7.53361434e-02,\n",
       "        -5.36606610e-02, -6.91469461e-02,  3.31886373e-02,\n",
       "        -2.43292581e-02, -5.06511070e-02, -4.21646051e-02,\n",
       "        -2.88240500e-02,  1.00347854e-03,  1.23253418e-02,\n",
       "        -7.22472295e-02,  4.75230590e-02, -4.27448638e-02,\n",
       "         2.77078222e-03,  2.95197740e-02,  1.43667813e-02,\n",
       "        -2.87530906e-02, -3.49552594e-02,  5.99354841e-02,\n",
       "        -4.56110947e-02, -4.72495928e-02, -4.57415581e-02,\n",
       "        -3.81943136e-02,  3.85272391e-02, -2.49122139e-02,\n",
       "         9.83435381e-03,  2.03819387e-02,  7.99411088e-02,\n",
       "        -5.37376292e-02,  3.29349190e-02,  1.11745195e-02,\n",
       "         3.79647354e-05,  6.28444999e-02, -5.14556281e-02,\n",
       "        -4.87262122e-02,  1.89580079e-02, -2.24726424e-02,\n",
       "         9.13025532e-03,  7.16648549e-02,  6.61652610e-02,\n",
       "         6.20802492e-02, -4.49803546e-02, -5.14746923e-03,\n",
       "         1.43364314e-02,  1.27019659e-02, -3.06474399e-02,\n",
       "         3.77705805e-02, -1.55364955e-02, -3.02082766e-02,\n",
       "         3.04549672e-02, -4.91915010e-02,  1.76575929e-02,\n",
       "         1.89216360e-02, -4.03109752e-02,  2.50492860e-02,\n",
       "        -2.64116600e-02,  1.25574721e-02,  8.63712747e-03,\n",
       "         7.05617666e-02,  1.46154445e-02,  5.84375151e-02,\n",
       "         5.82893603e-02, -7.16211125e-02, -3.32859781e-04,\n",
       "        -2.22963076e-02, -4.07282589e-03,  4.22376487e-03,\n",
       "         6.92200959e-02, -4.28796858e-02, -1.53849404e-02,\n",
       "        -7.59255886e-02,  2.36168709e-02,  5.64954504e-02,\n",
       "         3.38702649e-02, -1.22339185e-02, -2.81947479e-02,\n",
       "        -4.28066030e-02, -6.60772622e-02, -2.78431810e-02,\n",
       "         5.56712449e-02, -9.96780489e-03,  4.77544554e-02,\n",
       "         4.71426584e-02, -1.35595608e-03,  3.10320947e-02,\n",
       "        -1.72958020e-02, -2.66696904e-02,  3.81153896e-02,\n",
       "        -7.65143484e-02, -1.63193606e-02,  1.14397882e-02,\n",
       "        -3.46850269e-02, -1.23423645e-02,  1.84045881e-02,\n",
       "        -7.33401701e-02,  4.33905683e-02, -4.61264402e-02,\n",
       "         3.75244133e-02, -8.87119398e-03,  1.11249760e-02,\n",
       "         3.31228003e-02,  3.30728553e-02,  2.44284403e-02,\n",
       "         4.37344844e-03,  3.82500067e-02,  7.10265785e-02,\n",
       "        -5.02741430e-03,  3.86836082e-02, -1.07967211e-02,\n",
       "        -3.73508707e-02, -6.13486543e-02,  1.15152001e-02,\n",
       "         4.02764939e-02,  3.94785069e-02, -2.74731457e-04,\n",
       "         3.01068146e-02,  3.60189863e-02, -6.37078518e-03,\n",
       "         3.71119305e-02, -9.18050949e-03,  4.58870530e-02,\n",
       "        -4.10678573e-02, -7.83641934e-02, -3.77737731e-02,\n",
       "        -3.20528112e-02,  1.00177087e-01, -6.36946112e-02,\n",
       "        -4.32477891e-02, -6.60919724e-03,  2.37202886e-02,\n",
       "        -4.28364053e-03,  2.05248017e-02,  2.26792991e-02,\n",
       "        -8.15324765e-03, -3.24418470e-02,  9.79896113e-02,\n",
       "         6.61640838e-02,  2.18343679e-02, -2.48010573e-03,\n",
       "         7.47742057e-02,  3.48645151e-02,  2.07345914e-02,\n",
       "        -8.56059231e-03, -2.55001765e-02,  7.87802041e-02,\n",
       "        -4.56954986e-02, -4.30152975e-02,  7.15419129e-02,\n",
       "         5.46372570e-02, -5.72429299e-02, -2.59913132e-02,\n",
       "         1.57870762e-02, -4.97328527e-02, -3.95702422e-02,\n",
       "         3.25683579e-02,  7.12217614e-02,  5.05477674e-02,\n",
       "        -1.44158793e-03, -1.10715769e-01, -8.65989402e-02,\n",
       "         4.85498458e-02,  2.28010230e-02,  3.71048413e-02,\n",
       "         7.81487266e-04, -6.49653450e-02, -9.39159002e-03,\n",
       "        -4.03358601e-02, -5.23700118e-02,  1.78787205e-02,\n",
       "         7.30829267e-03,  5.06678708e-02,  3.04411184e-02,\n",
       "        -1.93953272e-02, -7.86726326e-02,  4.46686707e-02,\n",
       "        -8.89489129e-02,  1.29168453e-02,  2.96076946e-02,\n",
       "        -1.52417487e-02, -7.71390321e-03,  3.46271843e-02,\n",
       "        -1.63481589e-02,  3.66119482e-02,  2.54166294e-02,\n",
       "         6.07251301e-02,  3.76869924e-03, -2.63640173e-02,\n",
       "        -7.44659454e-03,  1.96817499e-02, -3.00363954e-02,\n",
       "        -4.06539962e-02, -8.62107649e-02, -7.93070905e-03,\n",
       "        -6.30771741e-02, -5.46475165e-02, -6.53310493e-02,\n",
       "        -3.69131193e-02,  2.53329948e-02, -1.91179961e-02,\n",
       "         5.09124361e-02, -1.19362786e-01,  2.94196680e-02,\n",
       "        -3.64400633e-02, -6.05981387e-02,  4.67517599e-02,\n",
       "         1.88597068e-02,  3.68920341e-02, -4.48148362e-02,\n",
       "        -4.20231521e-02, -4.20422181e-02, -1.26056150e-02,\n",
       "        -7.27929035e-03, -5.90543896e-02,  1.02987640e-01,\n",
       "         3.62535263e-03,  3.62087972e-02, -3.40934768e-02,\n",
       "        -1.59581825e-02,  3.89810395e-03, -4.75073606e-02,\n",
       "         3.44004296e-02,  2.53805872e-02,  1.23102618e-02,\n",
       "        -6.09837919e-02,  6.29641414e-02, -1.39104696e-02,\n",
       "         2.28737984e-02, -4.78222407e-02, -2.57978011e-02,\n",
       "         2.02198122e-02,  7.96239525e-02,  2.05394328e-02,\n",
       "         7.02668652e-02, -3.77013162e-02,  5.25800162e-04,\n",
       "         6.69259727e-02,  6.34444132e-02,  4.15574759e-02,\n",
       "         4.33153696e-02,  6.02175258e-02, -4.87883613e-02,\n",
       "         3.29188700e-03,  6.00611866e-02, -5.79778291e-02,\n",
       "        -2.18846127e-02, -1.28513845e-02, -5.64502589e-02,\n",
       "        -3.06796562e-02, -7.93625340e-02, -1.15407482e-02,\n",
       "         3.61782312e-02, -2.68380009e-02, -4.89818715e-02,\n",
       "         7.10874572e-02,  7.17928559e-02,  8.80630240e-02,\n",
       "         1.85423531e-02, -7.11321756e-02]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = hub.Module('./embeddings/GUSE')\n",
    "test_messages = [\"The quick brown fox jumped over the lazy dog\"]\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    message_embeddings = session.run(embed(test_messages))\n",
    "message_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model\n",
    "The code below creates a function which will take an input, pass it through GUSE, and return the output. Creating the function is necessary because of the way Keras works with TensorFlow modules. The next cell builds the model using Keras and displays the structure of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GUSE(param):\n",
    "    return embed(tf.squeeze(tf.cast(param, tf.string)), signature=\"default\", as_dict=True)[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0505 19:30:05.388706 17336 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "lambda_5 (Lambda)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 189,057\n",
      "Trainable params: 189,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(1,), dtype=\"string\")\n",
    "guse = Lambda(GUSE, output_shape=(512,))(input_layer)\n",
    "dense1 = Dense(256, activation=\"tanh\", kernel_regularizer=l2(0.01))(guse)\n",
    "dropout1 = Dropout(0.3)(dense1)\n",
    "dense2 = Dense(128, activation=\"tanh\", kernel_regularizer=l2(0.01))(dropout1)\n",
    "dropout2 = Dropout(0.3)(dense2)\n",
    "dense3 = Dense(128, activation=\"tanh\", kernel_regularizer=l2(0.01))(dropout2)\n",
    "dropout3 = Dropout(0.3)(dense3)\n",
    "dense4 = Dense(64, activation=\"tanh\", kernel_regularizer=l2(0.01))(dropout3)\n",
    "dropout4 = Dropout(0.3)(dense4)\n",
    "output = Dense(1, activation=\"sigmoid\")(dropout4)\n",
    "model = Model(inputs=[input_layer], outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "The code below trains the model for 35 epochs (i.e. 35 runs through the training data). It then saves the weight parameters to a `.h5` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1014 samples, validate on 113 samples\n",
      "Epoch 1/35\n",
      "1014/1014 [==============================] - 5s 5ms/step - loss: 7.6679 - acc: 0.6272 - val_loss: 7.0975 - val_acc: 1.0000\n",
      "Epoch 2/35\n",
      "1014/1014 [==============================] - 1s 872us/step - loss: 6.9805 - acc: 0.6854 - val_loss: 6.3299 - val_acc: 1.0000\n",
      "Epoch 3/35\n",
      "1014/1014 [==============================] - 1s 834us/step - loss: 6.3264 - acc: 0.6864 - val_loss: 5.6863 - val_acc: 1.0000\n",
      "Epoch 4/35\n",
      "1014/1014 [==============================] - 1s 819us/step - loss: 5.7146 - acc: 0.6874 - val_loss: 5.1468 - val_acc: 1.0000\n",
      "Epoch 5/35\n",
      "1014/1014 [==============================] - 1s 859us/step - loss: 5.1377 - acc: 0.7406 - val_loss: 4.6739 - val_acc: 1.0000\n",
      "Epoch 6/35\n",
      "1014/1014 [==============================] - 1s 871us/step - loss: 4.6112 - acc: 0.8077 - val_loss: 4.2100 - val_acc: 0.9735\n",
      "Epoch 7/35\n",
      "1014/1014 [==============================] - 1s 814us/step - loss: 4.1217 - acc: 0.8580 - val_loss: 3.7578 - val_acc: 0.9646\n",
      "Epoch 8/35\n",
      "1014/1014 [==============================] - 1s 822us/step - loss: 3.6859 - acc: 0.8728 - val_loss: 3.3695 - val_acc: 0.9381\n",
      "Epoch 9/35\n",
      "1014/1014 [==============================] - 1s 833us/step - loss: 3.3047 - acc: 0.8895 - val_loss: 3.0746 - val_acc: 0.9204\n",
      "Epoch 10/35\n",
      "1014/1014 [==============================] - 1s 848us/step - loss: 2.9674 - acc: 0.9014 - val_loss: 2.7711 - val_acc: 0.9204\n",
      "Epoch 11/35\n",
      "1014/1014 [==============================] - 1s 861us/step - loss: 2.6635 - acc: 0.9034 - val_loss: 2.4711 - val_acc: 0.9381\n",
      "Epoch 12/35\n",
      "1014/1014 [==============================] - 1s 822us/step - loss: 2.4003 - acc: 0.9172 - val_loss: 2.2877 - val_acc: 0.9204\n",
      "Epoch 13/35\n",
      "1014/1014 [==============================] - 1s 861us/step - loss: 2.1647 - acc: 0.9290 - val_loss: 2.0352 - val_acc: 0.9381\n",
      "Epoch 14/35\n",
      "1014/1014 [==============================] - 1s 764us/step - loss: 1.9534 - acc: 0.9300 - val_loss: 1.8720 - val_acc: 0.9381\n",
      "Epoch 15/35\n",
      "1014/1014 [==============================] - 1s 839us/step - loss: 1.7699 - acc: 0.9418 - val_loss: 1.6977 - val_acc: 0.9381\n",
      "Epoch 16/35\n",
      "1014/1014 [==============================] - 1s 834us/step - loss: 1.6090 - acc: 0.9389 - val_loss: 1.5193 - val_acc: 0.9469\n",
      "Epoch 17/35\n",
      "1014/1014 [==============================] - 1s 837us/step - loss: 1.4717 - acc: 0.9398 - val_loss: 1.4514 - val_acc: 0.9204\n",
      "Epoch 18/35\n",
      "1014/1014 [==============================] - 1s 855us/step - loss: 1.3368 - acc: 0.9438 - val_loss: 1.2649 - val_acc: 0.9469\n",
      "Epoch 19/35\n",
      "1014/1014 [==============================] - 1s 842us/step - loss: 1.2265 - acc: 0.9359 - val_loss: 1.2518 - val_acc: 0.9115\n",
      "Epoch 20/35\n",
      "1014/1014 [==============================] - 1s 861us/step - loss: 1.1347 - acc: 0.9487 - val_loss: 1.1227 - val_acc: 0.9469\n",
      "Epoch 21/35\n",
      "1014/1014 [==============================] - 1s 830us/step - loss: 1.0436 - acc: 0.9349 - val_loss: 1.0133 - val_acc: 0.9469\n",
      "Epoch 22/35\n",
      "1014/1014 [==============================] - 1s 861us/step - loss: 0.9605 - acc: 0.9467 - val_loss: 1.0202 - val_acc: 0.9027\n",
      "Epoch 23/35\n",
      "1014/1014 [==============================] - 1s 838us/step - loss: 0.8922 - acc: 0.9458 - val_loss: 0.8696 - val_acc: 0.9469\n",
      "Epoch 24/35\n",
      "1014/1014 [==============================] - 1s 862us/step - loss: 0.8301 - acc: 0.9536 - val_loss: 0.8609 - val_acc: 0.9381\n",
      "Epoch 25/35\n",
      "1014/1014 [==============================] - 1s 785us/step - loss: 0.7728 - acc: 0.9586 - val_loss: 0.7921 - val_acc: 0.9469\n",
      "Epoch 26/35\n",
      "1014/1014 [==============================] - 1s 848us/step - loss: 0.7212 - acc: 0.9536 - val_loss: 0.7660 - val_acc: 0.9469\n",
      "Epoch 27/35\n",
      "1014/1014 [==============================] - 1s 848us/step - loss: 0.6828 - acc: 0.9615 - val_loss: 0.7389 - val_acc: 0.9381\n",
      "Epoch 28/35\n",
      "1014/1014 [==============================] - 1s 824us/step - loss: 0.6502 - acc: 0.9438 - val_loss: 0.6525 - val_acc: 0.9469\n",
      "Epoch 29/35\n",
      "1014/1014 [==============================] - 1s 847us/step - loss: 0.6088 - acc: 0.9576 - val_loss: 0.6941 - val_acc: 0.9115\n",
      "Epoch 30/35\n",
      "1014/1014 [==============================] - 1s 854us/step - loss: 0.5835 - acc: 0.9536 - val_loss: 0.5780 - val_acc: 0.9558\n",
      "Epoch 31/35\n",
      "1014/1014 [==============================] - 1s 831us/step - loss: 0.5432 - acc: 0.9556 - val_loss: 0.6627 - val_acc: 0.9027\n",
      "Epoch 32/35\n",
      "1014/1014 [==============================] - 1s 867us/step - loss: 0.5209 - acc: 0.9615 - val_loss: 0.5523 - val_acc: 0.9469\n",
      "Epoch 33/35\n",
      "1014/1014 [==============================] - 1s 835us/step - loss: 0.5015 - acc: 0.9566 - val_loss: 0.5755 - val_acc: 0.9469\n",
      "Epoch 34/35\n",
      "1014/1014 [==============================] - 1s 868us/step - loss: 0.4790 - acc: 0.9596 - val_loss: 0.5535 - val_acc: 0.9469\n",
      "Epoch 35/35\n",
      "1014/1014 [==============================] - 1s 869us/step - loss: 0.4606 - acc: 0.9586 - val_loss: 0.5062 - val_acc: 0.9469\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "K.set_session(session)\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(tf.tables_initializer())\n",
    "history = model.fit(x, y, batch_size=256, epochs=35, validation_split=0.1, shuffle=True)\n",
    "nonce = str(int(uuid.uuid4()))\n",
    "model.save_weights('./models/guse_model' + nonce + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Data Benchmarking\n",
    "The code below accesses the Twitter data from a `.pickle` file generated in the data collection stage. It then reshapes the tensor to be the right dimension for input into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_f = open(\"twitter_irony_all.pickle\", mode=\"rb\")\n",
    "twitter_data = pickle.load(twitter_f)\n",
    "a = np.zeros((156,1))\n",
    "b = np.ones((156,1))\n",
    "twitter_y = b\n",
    "twitter_x = np.asarray(twitter_data)\n",
    "twitter_x = np.reshape(twitter_x, (twitter_x.shape[0],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the mode on Twitter data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 0s 972us/step\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(twitter_x, twitter_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8269230738664285"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Data Analysis\n",
    "The cell below prints out the predictions of the Twitter data. All Tweets in the dataset are ironic, and the model should output a number close to 1 for each piece of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46093705],\n",
       "       [0.9621376 ],\n",
       "       [0.08942699],\n",
       "       [0.45609495],\n",
       "       [0.9356887 ],\n",
       "       [0.6575923 ],\n",
       "       [0.91780066],\n",
       "       [0.9667613 ],\n",
       "       [0.45876235],\n",
       "       [0.25410482],\n",
       "       [0.9322748 ],\n",
       "       [0.9208282 ],\n",
       "       [0.97486037],\n",
       "       [0.9665472 ],\n",
       "       [0.98970455],\n",
       "       [0.8555902 ],\n",
       "       [0.948432  ],\n",
       "       [0.4179261 ],\n",
       "       [0.97569805],\n",
       "       [0.9814073 ],\n",
       "       [0.9812176 ],\n",
       "       [0.8448669 ],\n",
       "       [0.9125563 ],\n",
       "       [0.8238864 ],\n",
       "       [0.9352235 ],\n",
       "       [0.39621782],\n",
       "       [0.9765047 ],\n",
       "       [0.91828734],\n",
       "       [0.9766654 ],\n",
       "       [0.8971828 ],\n",
       "       [0.5088721 ],\n",
       "       [0.9283205 ],\n",
       "       [0.8167162 ],\n",
       "       [0.78996825],\n",
       "       [0.9370711 ],\n",
       "       [0.4863331 ],\n",
       "       [0.909876  ],\n",
       "       [0.95483714],\n",
       "       [0.9757598 ],\n",
       "       [0.7747267 ],\n",
       "       [0.9347778 ],\n",
       "       [0.98853487],\n",
       "       [0.998448  ],\n",
       "       [0.666966  ],\n",
       "       [0.9376991 ],\n",
       "       [0.14414498],\n",
       "       [0.9661401 ],\n",
       "       [0.4392709 ],\n",
       "       [0.9654687 ],\n",
       "       [0.6185691 ],\n",
       "       [0.37770513],\n",
       "       [0.99668306],\n",
       "       [0.84029335],\n",
       "       [0.6886779 ],\n",
       "       [0.99020875],\n",
       "       [0.9986532 ],\n",
       "       [0.653103  ],\n",
       "       [0.7214768 ],\n",
       "       [0.9779328 ],\n",
       "       [0.9842794 ],\n",
       "       [0.9861945 ],\n",
       "       [0.27606118],\n",
       "       [0.03890961],\n",
       "       [0.8451359 ],\n",
       "       [0.45710748],\n",
       "       [0.7948815 ],\n",
       "       [0.90470093],\n",
       "       [0.9686085 ],\n",
       "       [0.9960524 ],\n",
       "       [0.92103815],\n",
       "       [0.02123642],\n",
       "       [0.92079127],\n",
       "       [0.8530463 ],\n",
       "       [0.56957924],\n",
       "       [0.9950368 ],\n",
       "       [0.20224114],\n",
       "       [0.8925216 ],\n",
       "       [0.7862032 ],\n",
       "       [0.97593915],\n",
       "       [0.8269093 ],\n",
       "       [0.9700642 ],\n",
       "       [0.5787659 ],\n",
       "       [0.86182296],\n",
       "       [0.9325966 ],\n",
       "       [0.98514116],\n",
       "       [0.73631936],\n",
       "       [0.9205062 ],\n",
       "       [0.96185094],\n",
       "       [0.6666329 ],\n",
       "       [0.9480107 ],\n",
       "       [0.82095116],\n",
       "       [0.61606556],\n",
       "       [0.8945005 ],\n",
       "       [0.95449644],\n",
       "       [0.6196773 ],\n",
       "       [0.99250656],\n",
       "       [0.7275156 ],\n",
       "       [0.6224767 ],\n",
       "       [0.9489182 ],\n",
       "       [0.9782624 ],\n",
       "       [0.9830188 ],\n",
       "       [0.7321146 ],\n",
       "       [0.9786427 ],\n",
       "       [0.03224872],\n",
       "       [0.01948399],\n",
       "       [0.85854757],\n",
       "       [0.95189357],\n",
       "       [0.99382895],\n",
       "       [0.98921263],\n",
       "       [0.13340215],\n",
       "       [0.98989546],\n",
       "       [0.6999954 ],\n",
       "       [0.73489237],\n",
       "       [0.922065  ],\n",
       "       [0.69493043],\n",
       "       [0.97576255],\n",
       "       [0.9196595 ],\n",
       "       [0.98887163],\n",
       "       [0.99587196],\n",
       "       [0.8814483 ],\n",
       "       [0.9604926 ],\n",
       "       [0.88681906],\n",
       "       [0.75960535],\n",
       "       [0.8921025 ],\n",
       "       [0.9249007 ],\n",
       "       [0.98619056],\n",
       "       [0.00461926],\n",
       "       [0.49047914],\n",
       "       [0.7712916 ],\n",
       "       [0.7157396 ],\n",
       "       [0.8089004 ],\n",
       "       [0.10431826],\n",
       "       [0.94923264],\n",
       "       [0.9553223 ],\n",
       "       [0.59867024],\n",
       "       [0.09289957],\n",
       "       [0.92485553],\n",
       "       [0.84681046],\n",
       "       [0.94563127],\n",
       "       [0.7438868 ],\n",
       "       [0.8078848 ],\n",
       "       [0.5571787 ],\n",
       "       [0.94753563],\n",
       "       [0.99114335],\n",
       "       [0.96530515],\n",
       "       [0.01267113],\n",
       "       [0.95435166],\n",
       "       [0.9882125 ],\n",
       "       [0.008289  ],\n",
       "       [0.9515408 ],\n",
       "       [0.97153026],\n",
       "       [0.1362151 ],\n",
       "       [0.97101915],\n",
       "       [0.9153539 ],\n",
       "       [0.00503625],\n",
       "       [0.9815155 ]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(twitter_x)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2nd example (at index 1) appears to be accurately labeled by the model as ironic. The following line prints out the Tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chinese alchemists discovered gunpowder while searching for the elixir of immortality '],\n",
       "      dtype='<U137')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 18th example (at index 17) appears to be inaccurately labeled by the model. The following line prints out the Tweet. It is likely the model didn't do well because this example contains a sentence fragment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Am proofreading a style guide. \"Don\\'t use negatives\" apparently '],\n",
       "      dtype='<U137')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_x[17]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
